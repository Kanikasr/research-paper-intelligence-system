{
  "2014": [
    [
      "attention",
      142
    ],
    [
      "layer",
      60
    ],
    [
      "sequence",
      56
    ],
    [
      "layers",
      50
    ],
    [
      "decoder",
      48
    ],
    [
      "output",
      48
    ],
    [
      "models",
      46
    ],
    [
      "encoder",
      46
    ],
    [
      "model",
      44
    ],
    [
      "self",
      44
    ]
  ],
  "2018": [
    [
      "model",
      124
    ],
    [
      "bert",
      112
    ],
    [
      "tasks",
      88
    ],
    [
      "task",
      88
    ],
    [
      "training",
      86
    ],
    [
      "token",
      86
    ],
    [
      "tuning",
      86
    ],
    [
      "trained",
      82
    ],
    [
      "sentence",
      78
    ],
    [
      "language",
      60
    ]
  ]
}